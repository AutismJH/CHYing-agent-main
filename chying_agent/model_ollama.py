"""
Ollama æ¨¡å‹é€‚é…å™¨
==================

æä¾›æœ¬åœ° Ollama æ¨¡å‹çš„é›†æˆæ”¯æŒã€‚
"""
import logging
from typing import Optional
from langchain_core.language_models import BaseChatModel
from chying_agent.common import log_system_event


def create_ollama_model(
    base_url: str = "http://192.168.10.117:11434",
    model: str = "deepseek-r1:32b",
    temperature: float = 0.5,
    num_ctx: int = 8192,
    timeout: int = 300,
    num_predict: int = 4096
) -> BaseChatModel:
    """åˆ›å»º Ollama æ¨¡å‹å®ä¾‹"""
    try:
        from langchain_ollama import ChatOllama
    except ImportError as e:
        raise ImportError(
            "langchain-ollama æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install langchain-ollama"
        ) from e
    
    log_system_event(
        "âœ… åˆ›å»º Ollama æ¨¡å‹å®ä¾‹",
        {
            "base_url": base_url,
            "model": model,
            "temperature": temperature,
            "num_ctx": num_ctx,
            "timeout": timeout,
            "num_predict": num_predict
        }
    )
    
    try:
        model_instance = ChatOllama(
            base_url=base_url,
            model=model,
            temperature=temperature,
            num_ctx=num_ctx,
            timeout=timeout,
            num_predict=num_predict
        )
        
        _verify_ollama_connection(base_url, model)
        
        return model_instance
    
    except Exception as e:
        log_system_event(
            f"âŒ Ollama æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}",
            {"base_url": base_url, "model": model},
            level=logging.ERROR
        )
        raise ConnectionError(
            f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨ ({base_url})ã€‚\n"
            f"è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”æ¨¡å‹å·²ä¸‹è½½ã€‚"
        ) from e


def _verify_ollama_connection(base_url: str, model: str) -> None:
    """éªŒè¯ Ollama è¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§"""
    import requests
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        response.raise_for_status()
        
        available_models = response.json().get("models", [])
        model_names = [m["name"] for m in available_models]
        
        if model not in model_names:
            log_system_event(
                f"âš ï¸ è­¦å‘Šï¼šæ¨¡å‹ '{model}' æœªæ‰¾åˆ°",
                {
                    "available_models": model_names[:5],
                    "suggestion": f"è¿è¡Œ 'ollama pull {model}' ä¸‹è½½æ¨¡å‹"
                },
                level=logging.WARNING
            )
        else:
            log_system_event(f"âœ… Ollama æ¨¡å‹ '{model}' éªŒè¯æˆåŠŸ")
    
    except requests.exceptions.RequestException as e:
        raise ConnectionError(
            f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨ ({base_url}): {str(e)}"
        ) from e


def list_available_ollama_models(base_url: str = "http://192.168.10.117:11434") -> list:
    """åˆ—å‡ºå¯ç”¨çš„ Ollama æ¨¡å‹"""
    import requests
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        response.raise_for_status()
        
        models = response.json().get("models", [])
        model_info = []
        
        for model in models:
            model_info.append({
                "name": model["name"],
                "size": f"{model['size'] / 1e9:.2f} GB",
                "quantization": model["details"].get("quantization_level", "unknown")
            })
        
        log_system_event(
            f"ğŸ“‹ Ollama å¯ç”¨æ¨¡å‹åˆ—è¡¨",
            {"count": len(model_info), "models": model_info}
        )
        
        return model_info
    
    except Exception as e:
        log_system_event(
            f"âŒ è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}",
            level=logging.ERROR
        )
        return []
