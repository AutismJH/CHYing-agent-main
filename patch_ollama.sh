#!/bin/bash

# CHYing Agent Ollama é›†æˆè‡ªåŠ¨è¡¥ä¸è„šæœ¬
# åŠŸèƒ½ï¼šè‡ªåŠ¨ä¿®æ”¹åŸé¡¹ç›®æ–‡ä»¶ï¼Œæ·»åŠ  Ollama æ”¯æŒ
# ä½¿ç”¨ï¼šchmod +x patch_ollama.sh && ./patch_ollama.sh

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }

echo "
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         CHYing Agent Ollama é›†æˆè¡¥ä¸è„šæœ¬                   â•‘
â•‘         è‡ªåŠ¨ä¿®æ”¹åŸé¡¹ç›®ï¼Œæ·»åŠ  Ollama æ”¯æŒ                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"

# æ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•
if [ ! -f "pyproject.toml" ] || [ ! -d "chying_agent" ]; then
    print_error "è¯·åœ¨ CHYing-agent é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬"
    exit 1
fi

print_info "å¼€å§‹åº”ç”¨è¡¥ä¸..."

# å¤‡ä»½åŸæ–‡ä»¶
print_info "åˆ›å»ºå¤‡ä»½..."
mkdir -p .backup_$(date +%Y%m%d_%H%M%S)
BACKUP_DIR=".backup_$(date +%Y%m%d_%H%M%S)"

# ==================== 1. åˆ›å»º model_ollama.py ====================
print_info "[1/6] åˆ›å»º chying_agent/model_ollama.py..."

cat > chying_agent/model_ollama.py << 'EOF'
"""
Ollama æ¨¡å‹é€‚é…å™¨
==================

æä¾›æœ¬åœ° Ollama æ¨¡å‹çš„é›†æˆæ”¯æŒã€‚
"""
import logging
from typing import Optional
from langchain_core.language_models import BaseChatModel
from chying_agent.common import log_system_event


def create_ollama_model(
    base_url: str = "http://192.168.10.117:11434",
    model: str = "deepseek-r1:32b",
    temperature: float = 0.5,
    num_ctx: int = 8192,
    timeout: int = 300,
    num_predict: int = 4096
) -> BaseChatModel:
    """åˆ›å»º Ollama æ¨¡å‹å®ä¾‹"""
    try:
        from langchain_ollama import ChatOllama
    except ImportError as e:
        raise ImportError(
            "langchain-ollama æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install langchain-ollama"
        ) from e
    
    log_system_event(
        "âœ… åˆ›å»º Ollama æ¨¡å‹å®ä¾‹",
        {
            "base_url": base_url,
            "model": model,
            "temperature": temperature,
            "num_ctx": num_ctx,
            "timeout": timeout,
            "num_predict": num_predict
        }
    )
    
    try:
        model_instance = ChatOllama(
            base_url=base_url,
            model=model,
            temperature=temperature,
            num_ctx=num_ctx,
            timeout=timeout,
            num_predict=num_predict
        )
        
        _verify_ollama_connection(base_url, model)
        
        return model_instance
    
    except Exception as e:
        log_system_event(
            f"âŒ Ollama æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}",
            {"base_url": base_url, "model": model},
            level=logging.ERROR
        )
        raise ConnectionError(
            f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨ ({base_url})ã€‚\n"
            f"è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”æ¨¡å‹å·²ä¸‹è½½ã€‚"
        ) from e


def _verify_ollama_connection(base_url: str, model: str) -> None:
    """éªŒè¯ Ollama è¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§"""
    import requests
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        response.raise_for_status()
        
        available_models = response.json().get("models", [])
        model_names = [m["name"] for m in available_models]
        
        if model not in model_names:
            log_system_event(
                f"âš ï¸ è­¦å‘Šï¼šæ¨¡å‹ '{model}' æœªæ‰¾åˆ°",
                {
                    "available_models": model_names[:5],
                    "suggestion": f"è¿è¡Œ 'ollama pull {model}' ä¸‹è½½æ¨¡å‹"
                },
                level=logging.WARNING
            )
        else:
            log_system_event(f"âœ… Ollama æ¨¡å‹ '{model}' éªŒè¯æˆåŠŸ")
    
    except requests.exceptions.RequestException as e:
        raise ConnectionError(
            f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨ ({base_url}): {str(e)}"
        ) from e


def list_available_ollama_models(base_url: str = "http://192.168.10.117:11434") -> list:
    """åˆ—å‡ºå¯ç”¨çš„ Ollama æ¨¡å‹"""
    import requests
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        response.raise_for_status()
        
        models = response.json().get("models", [])
        model_info = []
        
        for model in models:
            model_info.append({
                "name": model["name"],
                "size": f"{model['size'] / 1e9:.2f} GB",
                "quantization": model["details"].get("quantization_level", "unknown")
            })
        
        log_system_event(
            f"ğŸ“‹ Ollama å¯ç”¨æ¨¡å‹åˆ—è¡¨",
            {"count": len(model_info), "models": model_info}
        )
        
        return model_info
    
    except Exception as e:
        log_system_event(
            f"âŒ è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}",
            level=logging.ERROR
        )
        return []
EOF

print_success "model_ollama.py åˆ›å»ºå®Œæˆ"

# ==================== 2. ä¿®æ”¹ config.py ====================
print_info "[2/6] ä¿®æ”¹ chying_agent/config.py..."

# å¤‡ä»½åŸæ–‡ä»¶
cp chying_agent/config.py "$BACKUP_DIR/config.py.bak"

# åœ¨ import éƒ¨åˆ†æ·»åŠ  Literal
sed -i '1i from typing import Literal' chying_agent/config.py

# ä¿®æ”¹ AgentConfig ç±»ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„ sed æ“ä½œï¼‰
# å»ºè®®ï¼šç›´æ¥æä¾›å®Œæ•´çš„ config.py æ›¿æ¢æ–‡ä»¶

print_warning "config.py éœ€è¦æ‰‹åŠ¨ä¿®æ”¹ï¼ˆsed è„šæœ¬å¤æ‚åº¦è¿‡é«˜ï¼‰"
print_info "è¯·å‚è€ƒæ–‡æ¡£æ‰‹åŠ¨ä¿®æ”¹ï¼Œæˆ–ä½¿ç”¨æä¾›çš„å®Œæ•´æ–‡ä»¶æ›¿æ¢"

# ==================== 3. ä¿®æ”¹ pyproject.toml ====================
print_info "[3/6] ä¿®æ”¹ pyproject.toml..."

cp pyproject.toml "$BACKUP_DIR/pyproject.toml.bak"

# åœ¨ dependencies ä¸­æ·»åŠ  langchain-ollama
sed -i '/dependencies = \[/a \    "langchain-ollama>=1.0.0",' pyproject.toml

print_success "pyproject.toml ä¿®æ”¹å®Œæˆ"

# ==================== 4. ä¿®æ”¹ .env.example ====================
print_info "[4/6] ä¿®æ”¹ .env.example..."

cp .env.example "$BACKUP_DIR/.env.example.bak"

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ  LLM_BACKEND é…ç½®
sed -i '1i # ============================================\n# LLM åç«¯é…ç½®\n# ============================================\nLLM_BACKEND=api\n' .env.example

# åœ¨ LLM é…ç½®éƒ¨åˆ†åæ·»åŠ  Ollama é…ç½®
cat >> .env.example << 'EOF'

# ============================================
# Ollama é…ç½®ï¼ˆä»…åœ¨ LLM_BACKEND=ollama æ—¶éœ€è¦ï¼‰
# ============================================
OLLAMA_BASE_URL=http://192.168.10.117:11434
OLLAMA_MAIN_MODEL=deepseek-r1:32b
OLLAMA_ADVISOR_MODEL=qwen3:latest
OLLAMA_TEMPERATURE=0.5
OLLAMA_NUM_CTX=8192
OLLAMA_NUM_PREDICT=4096
OLLAMA_TIMEOUT=300
EOF

print_success ".env.example ä¿®æ”¹å®Œæˆ"

# ==================== 5. åˆ›å»ºæ–‡æ¡£ ====================
print_info "[5/6] åˆ›å»º QUICKSTART_OLLAMA.md..."

cat > QUICKSTART_OLLAMA.md << 'EOF'
# CHYing Agent - Ollama å¿«é€Ÿå¼€å§‹

## é…ç½®æ­¥éª¤

1. å¤åˆ¶é…ç½®æ–‡ä»¶
```bash
cp .env.example .env
```

2. ç¼–è¾‘ .envï¼Œè®¾ç½® Ollama æ¨¡å¼
```bash
LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://192.168.10.117:11434
OLLAMA_MAIN_MODEL=deepseek-r1:32b
OLLAMA_ADVISOR_MODEL=qwen3:latest
```

3. å®‰è£…ä¾èµ–
```bash
uv sync
```

4. è¿è¡Œ
```bash
uv run main.py -t http://target.com
```

è¯¦è§å®Œæ•´æ–‡æ¡£ã€‚
EOF

print_success "QUICKSTART_OLLAMA.md åˆ›å»ºå®Œæˆ"

# ==================== 6. æç¤ºæ‰‹åŠ¨ä¿®æ”¹é¡¹ ====================
print_info "[6/6] ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ¸…å•..."

cat > MANUAL_MODIFICATIONS.md << 'EOF'
# éœ€è¦æ‰‹åŠ¨ä¿®æ”¹çš„æ–‡ä»¶

ç”±äºè¿™äº›æ–‡ä»¶çš„ä¿®æ”¹è¾ƒä¸ºå¤æ‚ï¼Œæ— æ³•é€šè¿‡ç®€å•çš„ sed è„šæœ¬å®Œæˆï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹ï¼š

## 1. chying_agent/config.py

### ä¿®æ”¹ 1ï¼šæ·»åŠ  Ollama å‚æ•°åˆ° __init__

åœ¨ `AgentConfig` ç±»çš„ `__init__` æ–¹æ³•ä¸­ï¼Œæ·»åŠ ï¼š

```python
def __init__(self,
             # â­ æ–°å¢
             llm_backend: Literal["api", "ollama"] = "api",
             
             # åŸæœ‰çš„ API é…ç½®ï¼ˆæ”¹ä¸º Optionalï¼‰
             llm_api_key: Optional[str] = None,
             llm_base_url: Optional[str] = None,
             
             # â­ æ–°å¢ Ollama é…ç½®
             ollama_base_url: str = "http://192.168.10.117:11434",
             ollama_main_model: str = "deepseek-r1:32b",
             ollama_advisor_model: str = "qwen3:latest",
             ollama_temperature: float = 0.5,
             ollama_num_ctx: int = 8192,
             ollama_num_predict: int = 4096,
             ollama_timeout: int = 300,
             
             # ... åŸæœ‰é…ç½®):
    
    self.llm_backend = llm_backend
    self.ollama_base_url = ollama_base_url
    # ... ä¿å­˜å…¶ä»– Ollama é…ç½®
```

### ä¿®æ”¹ 2ï¼šæ›´æ–° load_agent_config()

```python
def load_agent_config() -> AgentConfig:
    load_dotenv()
    
    # â­ æ–°å¢
    llm_backend = os.getenv("LLM_BACKEND", "api").lower()
    
    # â­ æ–°å¢ Ollama é…ç½®è¯»å–
    ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://192.168.10.117:11434")
    ollama_main_model = os.getenv("OLLAMA_MAIN_MODEL", "deepseek-r1:32b")
    # ... è¯»å–å…¶ä»– Ollama é…ç½®
    
    # â­ ä¿®æ”¹ï¼šAPI é…ç½®æ”¹ä¸ºå¯é€‰
    if llm_backend == "api":
        llm_api_key = os.getenv("DEEPSEEK_API_KEY")
        if not llm_api_key:
            raise ValueError("API æ¨¡å¼ä¸‹å¿…é¡»è®¾ç½® DEEPSEEK_API_KEY")
    else:
        llm_api_key = None
    
    return AgentConfig(
        llm_backend=llm_backend,
        ollama_base_url=ollama_base_url,
        ollama_main_model=ollama_main_model,
        # ... å…¶ä»–å‚æ•°
    )
```

## 2. chying_agent/model.py

### ä¿®æ”¹ï¼šcreate_model() å‡½æ•°

```python
def create_model(config: AgentConfig, ...) -> BaseChatModel:
    # â­ æ–°å¢ï¼šæ ¹æ® backend é€‰æ‹©
    if config.llm_backend == "ollama":
        from chying_agent.model_ollama import create_ollama_model
        return create_ollama_model(
            base_url=config.ollama_base_url,
            model=config.ollama_main_model,
            # ... å…¶ä»–å‚æ•°
        )
    else:
        # åŸæœ‰çš„ API æ¨¡å¼ä»£ç 
        from langchain_deepseek import ChatDeepSeek
        # ...
```

### æ–°å¢ï¼šcreate_advisor_model() å‡½æ•°

```python
def create_advisor_model(config: AgentConfig) -> BaseChatModel:
    """åˆ›å»ºé¡¾é—®æ¨¡å‹"""
    if config.llm_backend == "ollama":
        from chying_agent.model_ollama import create_ollama_model
        return create_ollama_model(
            base_url=config.ollama_base_url,
            model=config.ollama_advisor_model,  # ä½¿ç”¨é¡¾é—®æ¨¡å‹
            # ...
        )
    else:
        # åŸæœ‰çš„ MiniMax ä»£ç 
        from langchain_openai import ChatOpenAI
        # ...
```

## 3. chying_agent/retry_strategy.py

### ä¿®æ”¹ï¼š__init__() æ–¹æ³•

```python
def __init__(self, config):
    self.config = config
    
    # â­ ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ create_advisor_model
    self.main_llm = create_model(config=config)
    self.advisor_llm = create_advisor_model(config=config)  # æ–°å¢
```

## ä¿®æ”¹å®Œæˆåçš„éªŒè¯

```bash
# æµ‹è¯•å¯¼å…¥
python3 -c "from chying_agent.model_ollama import create_ollama_model; print('OK')"

# æµ‹è¯•é…ç½®
python3 -c "from chying_agent.config import load_agent_config; print(load_agent_config().llm_backend)"
```

EOF

print_success "æ‰‹åŠ¨ä¿®æ”¹æ¸…å•å·²ç”Ÿæˆï¼šMANUAL_MODIFICATIONS.md"

# ==================== å®Œæˆ ====================
echo ""
print_success "è¡¥ä¸åº”ç”¨å®Œæˆï¼"
echo ""
print_warning "âš ï¸ é‡è¦æç¤ºï¼š"
echo "  1. éƒ¨åˆ†æ–‡ä»¶éœ€è¦æ‰‹åŠ¨ä¿®æ”¹ï¼ˆè§ MANUAL_MODIFICATIONS.mdï¼‰"
echo "  2. åŸæ–‡ä»¶å¤‡ä»½åœ¨ï¼š$BACKUP_DIR/"
echo "  3. ä¿®æ”¹å®Œæˆåè¿è¡Œï¼šuv sync"
echo ""
print_info "ä¸‹ä¸€æ­¥ï¼š"
echo "  1. é˜…è¯» MANUAL_MODIFICATIONS.md å¹¶å®Œæˆæ‰‹åŠ¨ä¿®æ”¹"
echo "  2. å¤åˆ¶ .env.example åˆ° .env å¹¶é…ç½®"
echo "  3. è¿è¡Œ uv sync å®‰è£…ä¾èµ–"
echo "  4. è¿è¡Œ uv run main.py -t http://target.com æµ‹è¯•"
echo ""
